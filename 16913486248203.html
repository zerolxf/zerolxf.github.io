<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  二阶随机优化算法 - zeroxf
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="zeroxf" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
 
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:zerolxf.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_blank" href="index.html">Home</a></li>
        
        <li id=""><a target="_blank" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; zeroxf</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_blank" href="index.html">Home</a></li>
        
        <li><a target="_blank" href="archives.html">Archives</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="GPT%20%E8%AF%BB%E8%AE%BA%E6%96%87.html">GPT 读论文</a></li>
        
            <li><a href="%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8.html">配置使用</a></li>
        
            <li><a href="java.html">java</a></li>
        
            <li><a href="c++.html">c++</a></li>
        
            <li><a href="%E6%95%B0%E5%AD%A6&%E4%BC%98%E5%8C%96&%E7%AE%97%E6%B3%95.html">数学&优化&算法</a></li>
        
            <li><a href="python.html">python</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>二阶随机优化算法</h1>
     
        <div class="read-more clearfix">
          <span class="date">2023/08/07</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='%E6%95%B0%E5%AD%A6&%E4%BC%98%E5%8C%96&%E7%AE%97%E6%B3%95.html'>数学&优化&算法</a></span>
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <p>最近看了几篇二阶优化算法，现在总结一下，以便日后查阅</p>
<h2><a id="%E4%BA%8C%E9%98%B6%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>二阶算法</h2>
<blockquote>
<p>二阶优化算法又称为牛顿法，牛顿法是微积分学中， 通过迭代以求解可微函数f的零点的一种算法，而在最优化中，牛顿法通常被运用于求解一个二次可微函数f的一阶导数f’的零点x， 同时也是f的驻点。 因此从另一个角度而言，应用于最优化中的牛顿法是求解函数 f(x)的最小值或最大值的一种算法。</p>
</blockquote>
<p>二阶算法都是从牛顿法演变而来，关于牛顿法详情可以参考我另外一篇博客</p>
<span id="more"></span><!-- more -->
<h3><a id="%E4%BC%A0%E7%BB%9F%E4%BA%8C%E9%98%B6%E7%AE%97%E6%B3%95%E7%BC%BA%E9%99%B7" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>传统二阶算法缺陷</h3>
<p>传统的二阶优化方法如牛顿法，主要有两个缺陷，一个是需要计算Hessian矩阵，需要O(np^2) 的复杂度，另外一个便是计算Hessian的逆矩阵需要O(p^3 )的复杂度。而在p维度很高的时候，传统二阶优化法显然不能够适用。</p>
<h2><a id="%E9%9A%8F%E6%9C%BA%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>随机算法</h2>
<p>随机梯度下降类算法以及其改进SVRG、SAGA等算法的提出，大大降低了算法迭代一次所需要的时间。在最近的相关研究中，随机一阶优化算法已经在机器学习优化模型中占据主导支配地位，这很大是由于它能够在大规模的模型训练中提供可负担的线性计算成本。对一阶优化算法提升起到主要贡献的研究工作，其中包括adaptive regularization， variance reduction，dual coordinate ascent。与之对比，二阶优化算法由于每次迭代需要计算一次Hessian矩阵以及Hessian的逆，其一次迭代过高的计算复杂性和消耗大量内存导致不能够适用于大规模的机器学习应用之中。</p>
<p>近期，让二阶算法适用于大规模机器学习中，有了相关进展工作。其中主要典型的研究工作有（包括但不局限于以下）：1.Stochastic L-BFGS 2. NewSamp 3.LiSSA。他们都是从不同角度解决二阶方法的局限性，下面将分别介绍这三种方法。</p>
<h2><a id="newsamp" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>NewSamp</h2>
<blockquote>
<p>A Newton method via sub-sampling and eigenvalue thresholding</p>
</blockquote>
<p>这里的NewSamp核心想法就是对Hessain子采样(sub-sampling)以及低秩矩阵近似(low-rank approximation),这里为什么可以低秩近似?需要先对奇异值含义有所理解,可以参考知乎问题<br />
<a href="https://www.zhihu.com/question/22237507">奇异值的物理意义是什么？</a></p>
<h3><a id="newsamp%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>NewSamp算法分析</h3>
<p>NewSamp他实质是基于子采样的牛顿法，是一个很简洁的算法。</p>
<ol>
<li>首先，NewSamp通过子采样避免对所有样本求Hessian。</li>
<li>对于Hessian求逆. 牛顿法主要通过发掘Hessian的中所包含的曲率信息达到加速效果，由于重要的二阶曲率信息一般包含在最大的若干个特征值以及其对应的特征向量中。因而对于Hessian求逆的问题，NewSamp采用低秩矩阵近似技术来得到Hessian矩阵逆的近似。假如目标函数是凸函数，那么对应的Hessian矩阵特征值便是非负，对称的Hessian矩阵的奇异值和特征值相同，因而算法NewSamp采用截断SVD分解获得前k大的特征值以及对应特征向量, 然后快速得到Hessian的逆。</li>
</ol>
<h3><a id="%E8%B4%A1%E7%8C%AE" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>贡献</h3>
<p>NewSamp的最主要贡献，是在提供良好收敛效果的同时提供了理论保证。</p>
<h3><a id="%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>算法过程</h3>
<p><img src="http://oh9ex6wd2.bkt.clouddn.com/31.png" alt="NewSamp" /></p>
<p>这里作者并没有直接套用最基本的rank-r approximation</p>
<blockquote>
<p>To construct the curvature matrix [Qt]−1, instead of using the basic rank-r approximation, we fill its 0 eigenvalues with the (r+1)-th eigenvalue of the sub-sampled Hessian which is the largest eigenvalue below the threshold.</p>
</blockquote>
<p>这里如果我没有理解错的话,应该是让奇异值小于(r+1)以后的奇异值都变为第(r+1)个奇异值,黄色高亮部分就是这句话优雅的数学表达形式</p>
<p>这里的投影操作实际可以不用,用了只会效果更好,作者在后面的理论证明中直接跳过了投影这个操作</p>
<h3><a id="%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>实验结果</h3>
<p>表现最好的那个就是NewSamp<br />
<img src="http://oh9ex6wd2.bkt.clouddn.com/32.png" alt="NewSamp2" /></p>
<h2><a id="stochastic-l-bfgs%E7%AE%97%E6%B3%95" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic L-BFGS算法</h2>
<blockquote>
<p><em>the limited-memory version of the classic BFGS algorithm</em></p>
</blockquote>
<p>在牛顿法的迭代中， 需要计算Hessian矩阵的逆矩阵这一计算比较复杂，考虑用一个n阶矩阵\(G_k\)近似代替\(H_k^{-1}\)。这就是拟牛顿法中基本想法，BFGS算法是最流行的拟牛顿算法。但是BFGS算法需要存储上一次迭代中的Hessian，这消耗大量内存，因而改良版的基于Hessian-vector的Limited BFGS算法提出。.最新的论文<em>A Linearly-Convergent Stochastic L-BFGS Algorithm</em>中加入了<strong>variance reduction</strong>和<strong>随机优化</strong>, 让它能够处理一些稍大规模的优化问题,并且以较好的线性收敛</p>
<h3><a id="%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>算法分析</h3>
<p>在SVRG和SAGA算法提出后，随机算法各方面性能有了显著提升，基于L-BFGS的随机优化算法也相应提出，其在部分数据上的实验效果不亚于SVRG等主流算法。</p>
<p>个人感觉Stochastic L-BFGS主要由以下三点保证了他的优势：</p>
<ul>
<li>基于BFGS的拟牛顿法的良好算法性能</li>
<li>基于Hessian-vector思想而不必存储Hessian矩阵以及减少计算复杂度.</li>
<li>基于Variance Reduce的随机梯度估计矫正。</li>
</ul>
<p>但是他有明显的劣势，有很多参数需要设置。这里有一个值得关注的地方, SLBFGS算法他的Hessian是一个类似于增量不断更新的, 并且和方差约减后的梯度相乘</p>
<h3><a id="%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>算法流程</h3>
<p>每m次计算一次全梯度,来减少variance<br />
每L次算法更新一次Hessian矩阵，这里的更新利用了过去M次（Sj, Yj）的曲率信息<br />
Sr记录过去2L次的平均方向<br />
Yr是Sr和Hessian矩阵相乘得出的<br />
<img src="http://oh9ex6wd2.bkt.clouddn.com/21.png" alt="L-BFGS" /></p>
<p>这里黄色高亮的部分就是variance reduction,这里可以说是L-BFGS加入了<strong>variance reduction</strong>,也反过来说是<strong>variance reduction</strong>加入了二阶信息,而这里采用了随机优化后的L-BFGS</p>
<h2><a id="lissa" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lissa</h2>
<blockquote>
<p>LiSSA: Algorithm 1 is a practical stochastic second-order algorithm based on a novel estimator of the Hessian inverse, leading to an efficient approximate Newton step (Equation 1). The estimator is based on the well known Taylor approximation of the inverse</p>
</blockquote>
<p>Lissa 核心想法是通过Taylor来近似逆从而对Hessain逆进行采样<br />
<img src="http://oh9ex6wd2.bkt.clouddn.com/42.png" alt="Taylor approximation" /></p>
<h3><a id="%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>算法分析</h3>
<p>LiSSA是一个具有实际应用价值的二阶随机算法，他为二阶优化算法提出一种新颖的思路。NewSamp是对通过子采样估计出一个较为精确的Hessian，然后通过矩阵分解得出Hessian的逆的近似。随机版本的L-BFGS则是基于拟牛顿思路，通过构造得出满足拟牛顿条件并和原Hessian的逆尽可能接近的矩阵。但是LiSSA则是通过对Hessian的逆的泰勒展开式，得出Hessian逆与Hessian的等式，再通过对Hessian的进行采样估计，来直接估计出Hessian的逆。LiSSA的思路完全不同于上述两种算法，同时LiSSA算法采用Hessian-vector product对形式因而对广义线性模型具有更加明显的优势。</p>
<h3><a id="%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>算法流程</h3>
<p><img src="http://oh9ex6wd2.bkt.clouddn.com/41.png" alt="算法" /></p>
<p>这里S1表示采样数量的多少<br />
S2表示一个采样中Taylor近似的深度<br />
这里的X[i,j]表示的是梯度和 Hessain逆估计的乘积,是一个向量</p>
<p>高亮的部分其实就是之前Hessain逆的Taylor近似的递推形式,原理如下<br />
<img src="http://oh9ex6wd2.bkt.clouddn.com/43.png" alt="递推" /></p>
<p>这里递推初始化是梯度, 相当于在原来的递推式子上面都乘以了梯度, 最后得出的就是迭代量,拟牛顿法中的p</p>
<p>这里用X向量可以简化运算</p>
<h3><a id="lissa%E7%9A%84%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A7%8D%E7%90%86%E8%A7%A3" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lissa的另外一种理解</h3>
<p><img src="http://oh9ex6wd2.bkt.clouddn.com/45.png" alt="理解" /></p>
<p>这里通过\(y=x- x_{t-1}\) 以及替换f(x)中的x,  并在点\(x_{t-1}\)泰勒展开得到上式中的Q(y)表达式, 那么原本的求f(x)最小值等价于Q(y)最小值<br />
Q(y)相当于是对f(x)的二阶近似, 此处转化为对Q(y)的求最值, 然后这是一个二次函数, 我们可以用一般梯度下降算法对此处求最值, 也就是\(y_t^{i+1}\)的迭代式,这里他的形式和Lissa的迭代形式很像, 所以可以看做是Lissa的一种理解, 也可以理解Lissa能够获得线性收敛率的原因</p>
<h2><a id="%E4%BA%8C%E9%98%B6%E4%BC%98%E5%8C%96%E7%9A%84%E9%97%AE%E9%A2%98" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>二阶优化的问题</h2>
<p>这里是师兄对我提的问题,我觉得很有意思,需要理解才行</p>
<h3><a id="%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BA%8C%E9%98%B6%E6%96%B9%E6%B3%95%E5%BF%AB" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>为什么二阶方法快</h3>
<p><img src="http://oh9ex6wd2.bkt.clouddn.com/46.png" alt="图解" /><br />
在<em>Deep learning via Hessian-free optimization</em>中有简单提到, 当解处于一个类似于平原的时候, 普通梯度下降如果仍旧以原来的步伐走, 那么需要很久才能走出来, 如果此时乘以一个Hessain的逆, 那么可以想象这里的Hessain表示曲率. Hessain曲率偏小, 他的必然就会放大步伐</p>
<p>相反,如果梯度变化太快,如果仍旧原步伐, 很可能就来到一个很差的点, 乘以Hessain的逆,可以放慢步伐</p>
<h3><a id="%E6%96%87%E7%AB%A0%E4%B8%AD%E9%83%BD%E8%AF%B4%E6%98%AF%E8%80%83%E8%99%91%E8%BF%9B%E6%9D%A5%E4%BA%86%E6%9B%B2%E7%8E%87%E4%BF%A1%E6%81%AF%E6%98%AF%E6%80%8E%E4%B9%88%E8%80%83%E8%99%91%E7%9A%84%EF%BC%9F" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>文章中都说是考虑进来了曲率信息    是怎么考虑的？</h3>
<h3><a id="hessian%E7%9F%A9%E9%98%B5%E6%98%AF%E5%A6%82%E4%BD%95%E6%90%BA%E5%B8%A6%E6%9B%B2%E7%8E%87%E4%BF%A1%E6%81%AF%E7%9A%84%E9%A2%9D%EF%BC%9F" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hessian矩阵是如何携带曲率信息的额？</h3>
<p>参考文献</p>
<ol>
<li>《统计学习方法》 -李航</li>
<li>Second-Order Stochastic Optimization for Machine Learning in<br />
Linear Time</li>
<li>Second-Order Stochastic Optimization for Machine Learning in<br />
Linear Time</li>
<li>A Linearly-Convergent Stochastic L-BFGS Algorithm</li>
<li>Deep learning via Hessian-free optimization</li>
</ol>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="16913487231002.html" 
          title="Previous Post: 搭建正向反向代理">&laquo; 搭建正向反向代理</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="16913485508310.html" 
          title="Next Post: Pandas 读取文本数据">Pandas 读取文本数据 &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="https://avatars.githubusercontent.com/u/12774971?v=4" /></div>
            
                <h1>zeroxf</h1>
                <div class="site-des"></div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/zerolxf" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:liangxianfeng96@qq.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="GPT%20%E8%AF%BB%E8%AE%BA%E6%96%87.html"><strong>GPT 读论文</strong></a>
        
            <a href="%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8.html"><strong>配置使用</strong></a>
        
            <a href="java.html"><strong>java</strong></a>
        
            <a href="c++.html"><strong>c++</strong></a>
        
            <a href="%E6%95%B0%E5%AD%A6&%E4%BC%98%E5%8C%96&%E7%AE%97%E6%B3%95.html"><strong>数学&优化&算法</strong></a>
        
            <a href="python.html"><strong>python</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="16913487231002.html">搭建正向反向代理</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16913486248203.html">二阶随机优化算法</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16913485508310.html">Pandas 读取文本数据</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16913485233251.html">Pandas 数据整合</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16913485004157.html">Python 数据分析画图&one-hot编码</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  














<style type="text/css">
figure{margin: 0;padding: 0;}
figcaption{text-align:center;}

/* PrismJS 1.14.0
 http://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    text-shadow: 0 1px white;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
    
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
    
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
    text-shadow: none;
    background:#b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
    text-shadow: none;
    background: #b3d4fc;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #F7F7F7;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #999;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #9a6e3a;
    background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #07a;
}

.token.function,
.token.class-name {
    color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
    color: #e90;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }

</style>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>



  </body>
</html>
