---
title: UNILM Unified Pre-training for Language Understanding and Generation
date: 2023-08-8 00:00:00
tags: 大模型
categories:
- 大模型
---
# UNILM 
Unified Pre-training for Language Understanding and Generation
这篇论文的作者是Liu等人，发表于2019年。论文提出了一种新的统一预训练语言模型（UNILM），可以应用于自然语言理解和生成任务。

## 背景
预训练的语言模型已经在各种自然语言处理任务中取得了显著的进步。预训练的语言模型通过预测基于大量文本数据的上下文中的词来学习上下文化的文本表示，并可以进行微调以适应下游任务。然而，现有的预训练模型，如BERT，主要用于自然语言理解任务，而不适用于自然语言生成任务。

## 解决问题
本文提出了一种新的统一预训练语言模型（UNILM），可以应用于自然语言理解和生成任务。UNILM是一个多层Transformer网络，共同预训练在大量文本上，针对三种类型的无监督语言建模目标进行优化：单向LM，双向LM和序列到序列LM。为了控制预测条件的上下文，我们使用特定的自注意力掩码来控制共享的Transformer网络。
<!-- more -->
## 相关工作
ELMo、GPT和BERT等预训练模型在自然语言处理任务中取得了显著的进步。然而，这些模型主要用于自然语言理解任务，而不适用于自然语言生成任务。

## 核心方法和步骤
1. UNILM的预训练包括三种类型的语言建模任务：单向、双向和序列到序列预测。统一的建模是通过使用共享的Transformer网络和利用特定的自注意力掩码来控制预测条件的上下文来实现的。这种方法的好处是可以通过使用不同的自注意力掩码来配置，以聚合不同类型的语言模型的上下文，因此可以用于自然语言理解和生成任务。
2. UNILM的输入表示遵循BERT的方式。文本被分词为WordPiece单元。对于每个输入token，其向量表示是通过将相应的token嵌入、位置嵌入和段落嵌入相加来计算的。因为UNILM是使用多种LM任务进行训练的，所以段落嵌入也起到了LM标识符的作用，我们为不同的LM目标使用不同的段落嵌入。

## 工作对比
- 与BERT等模型不同，UNILM可以通过使用不同的自注意力掩码来配置，以聚合不同类型的语言模型的上下文，因此可以用于自然语言理解和生成任务。
- 实验结果表明，我们的模型作为双向编码器，与BERT在GLUE基准测试和两个抽取式问题回答任务（即SQuAD 2.0和CoQA）上的比较有利。此外，我们在五个自然语言生成数据集上展示了UNILM的有效性，其中它被用作序列到序列模型，在CNN/DailyMail和Gigaword抽象摘要，SQuAD问题生成，CoQA生成问题回答，和DSTC7对话响应生成上创造了新的最先进的结果。