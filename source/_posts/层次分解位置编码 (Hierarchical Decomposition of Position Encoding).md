---
title: 层次分解位置编码 (Hierarchical Decomposition of Position Encoding)
date: 2023-08-8 00:00:00
tags: 大模型 Transformer
categories:
- 大模型
---
# 层次分解位置编码 (Hierarchical Decomposition of Position Encoding)
论文标题: 层次分解位置编码，让BERT可以处理超长文本
作者: 苏剑林
时间: 2020-12-04
核心算法名: 层次分解位置编码
论文链接: [https://spaces.ac.cn/archives/7947](https://spaces.ac.cn/archives/7947)

## 背景
BERT模型最多能处理512个token的文本。这一瓶颈的根本原因是BERT使用了从随机初始化训练出来的绝对位置编码，一般的最大位置设为了512，因此顶多只能处理512个token，多出来的部分就没有位置编码可用了。另一个重要的原因是Attention的$O(n^2)$复杂度，导致长序列时显存用量大大增加，一般显卡也finetune不了。

## 解决问题
如何简单修改当前最大长度为512的BERT模型，使得它可以直接处理更长的文本。主要思路是层次分解已经训练好的绝对位置编码，使得它可以延拓到更长的位置。
<!-- more -->
## 相关工作
解决这个问题的一个主流思路是换成相对位置编码，这是个可行的办法，华为的NEZHA模型便是一个换成了相对位置编码的BERT模型。相对位置编码一般会对位置差做个截断，使得要处理的相对位置都在一个有限的范围内，因此相对位置编码可以不受限于序列长度。

## 核心方法和步骤
1. 层次分解已经训练好的绝对位置编码，使得它可以延拓到更长的位置。具体来说，假设已经训练好的绝对位置编码向量为$p_1,p_2,...,p_n$，我们希望能在此基础上构造一套新的编码向量$q_1,q_2,...,q_m$，其中$m > n$。为此，我们设
    - $q_{(i-1)×n + j} = αu_i + (1 - α)u_j$ 其中$α∈ (0, 1)$且$α ≠ 0.5$是一个超参数，$u_1,u_2,...,u_n$是该套位置编码的“基底”。这样的表示意义很清晰，就是将位置$(i - 1)×n + j$层次地表示为$(i, j)$，然后$i, j$对应的位置编码分别为$αu_i$和$(1 - α)u_j$，而最终$(i - 1)×n + j$的编码向量则是两者的叠加。
    - $u_i = \frac{p_i - αp_1}{1 - α}$，$i = 1,2,...,n$ 这样一来，我们的参数还是$p_1,p_2,...,p_n$，但我们可以表示出$n^2$个位置的编码，并且前$n$个位置编码跟原来模型是相容的。

## 工作对比
- 和其他工作一个典型工作的方法区别是什么: 与使用相对位置编码的方法不同，这种方法是在已经训练好的绝对位置编码的基础上进行层次分解，使得它可以延拓到更长的位置，而不需要重新训练模型。
- 效果对比: 实验验证了这种方法的有效性，它可以使BERT模型处理更长的文本。