---
title: T5-谷歌
date: 2023-08-8 00:00:00
tags: 大模型 
categories:
- 大模型
---
# T5-谷歌
论文标题: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", 作者: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu, 时间: 2019年10月, 核心算法名: T5 (Text-to-Text Transfer Transformer), 论文链接: [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)

## 背景
该论文的目标不是提出新的方法，而是提供一个全面的视角来理解当前领域的状况。因此，这项工作主要包括对现有技术的调查、探索和实证比较。作者还通过扩大系统研究的洞察力（训练多达110亿参数的模型）来探索当前方法的极限，从而在许多任务中获得了最先进的结果。为了进行这种规模的实验，作者引入了“巨大的清洁爬行语料库”（C4），这是一个由网络上抓取的数百GB的清洁英文文本组成的数据集。认识到转移学习的主要用途是在数据稀缺的环境中利用预训练模型，作者发布了他们的代码、数据集和预训练模型（Page 3）。

## 解决问题
该论文通过采用合理的基线（在第3.1节中描述），并一次改变设置的一个方面，系统地研究了这些贡献。例如，在第3.3节中，作者在保持实验流程不变的情况下，测量了不同无监督目标的性能。这种“坐标上升”方法可能会错过二阶效应（例如，某个特定的无监督目标可能在比基线设置更大的模型上效果最好），但进行所有可能组合的探索将是不切实际的（Page 10）。
<!-- more -->
## 相关工作
该论文的研究是在许多相关工作的基础上进行的，包括BERT、GPT-2、RoBERTa、XLNet等。这些工作都在一定程度上影响了T5的设计和实现。然而，T5的目标是通过统一的文本到文本框架，对这些方法进行全面的比较和分析，以理解各种技术的贡献和重要性（Page 10）。

## 核心方法和步骤
1. T5采用了文本到文本的框架，这提供了一种简单的方法来训练单一模型以处理各种文本任务，使用相同的损失函数和解码过程。这种方法可以成功应用于生成任务（如抽象摘要）、分类任务（如自然语言推理）甚至回归任务（如STS-B）。尽管简单，但文本到文本框架的性能与任务特定的架构相当，并最终在与规模结合时产生了最先进的结果（Page 41）。
2. 在无监督目标的探索中，作者发现去噪目标在预训练中优于语言建模和解混乱。他们没有观察到去噪目标的许多变体之间的显著差异。然而，不同的目标（或目标的参数化）可以导致不同的序列长度，从而导致不同的训练速度（Page 24-25）。

## 工作对比
- 与其他工作的方法区别是什么
  T5的主要区别在于它采用了统一的文本到文本框架，这使得它能够在各种任务上使用相同的模型和损失函数。此外，T5还进行了大规模的实验，以探索转移学习的极限，并在许多任务中达到了最先进的结果（Page 41）。

- 实验效果对比
  在最后的一组实验中，T5在24个任务中的18个任务上实现了最先进的性能。预期地，他们最大的（110亿参数）模型在所有任务中的性能最好。他们的T5-3B模型变体在几个任务中超越了之前的最先进的技术，但将模型规模扩大到110亿参数是实现最佳性能的最重要因素（Page 38）。